{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHOCV6ydM0FrHGbhHb2b7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Solenabera/AOGEC-BERT/blob/main/AOGEC_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ybsGCJ1Jbrtx",
        "outputId": "a7bb60be-3cb6-4bc0-ffe4-f56e889e3324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TIj4RigEJ62G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the T5 model and tokenizer\n",
        "model_name = \"t5-base\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "iWjLELnZK_yQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dcb9263-26d3-444d-8d98-da89f94558bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define your custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.data.iloc[index]\n",
        "        input_text = example['input']\n",
        "        target_text = example['target']\n",
        "\n",
        "        input_ids = self.tokenizer.encode(input_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        target_ids = self.tokenizer.encode(target_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids.squeeze(),\n",
        "            'attention_mask': input_ids.squeeze().gt(0),\n",
        "            'labels': target_ids.squeeze()\n",
        "        }"
      ],
      "metadata": {
        "id": "uSFnMn-NZ8vx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split your dataset into training and validation sets\n",
        "csv_file_path = 'Data - 04.csv'\n",
        "dataset = pd.read_csv(csv_file_path)\n",
        "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vGg_Mveog71x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create custom datasets for training and validation\n",
        "train_dataset = CustomDataset(train_data, tokenizer)\n",
        "val_dataset = CustomDataset(val_data, tokenizer)"
      ],
      "metadata": {
        "id": "I_82PwlLhKxA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Create data loaders for training and validation\n",
        "batch_size = 2  # Default 4\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "tjGrRBpvwdjL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Measure the loss function before training\n",
        "model.eval()\n",
        "\n",
        "total_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "average_loss = total_loss / len(val_dataloader)\n",
        "print(f'Loss before training: {average_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANfsIlHUwjKH",
        "outputId": "1746d5b4-22f1-4683-c678-d26020a6fb70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss before training: 16.2931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Fine-tune the model\n",
        "model.train()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "epochs = 1 # Default 10\n",
        "print(\"Training Started\")\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch: {epoch}')\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print(total_loss)\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch+1} - Average Loss: {average_loss:.4f}')\n",
        "\n",
        "print(\"Training Ended\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tI7BEtvzbOj",
        "outputId": "0de858ba-a3f8-4a90-d2f2-448692971262"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Started\n",
            "Epoch: 0\n",
            "13.257418632507324\n",
            "27.118528366088867\n",
            "42.441813468933105\n",
            "57.063599586486816\n",
            "65.26397705078125\n",
            "78.40618133544922\n",
            "Epoch 1 - Average Loss: 13.0677\n",
            "Training Ended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Measure the loss function after training\n",
        "model.eval()\n",
        "\n",
        "total_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "average_loss = total_loss / len(val_dataloader)\n",
        "print(f'Loss after training: {average_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87mYKAie_o4V",
        "outputId": "df06b5a2-3c5c-4415-c4de-dbe0280a43c2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after training: 12.4953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Save the fine-tuned model\n",
        "model.save_pretrained('OutputModel')"
      ],
      "metadata": {
        "id": "dfI2T_wH_1EM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained('OutputModel')\n",
        "csv_file_path = 'Data - 04.csv'\n",
        "dataset = pd.read_csv(csv_file_path)\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "test_dataset = CustomDataset(test_data, tokenizer)\n",
        "\n",
        "batch_size = 2  # Default 4\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "0eZVzsSY9QrF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 512"
      ],
      "metadata": {
        "id": "KLfcFptE_ILk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=MAX_LENGTH)\n",
        "        predicted_ids = outputs\n",
        "\n",
        "        predicted_texts = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "        label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        for predicted_text, label_text in zip(predicted_texts, label_texts):\n",
        "            print(f'Predicted: {predicted_text}')\n",
        "            print(f'Correct: {label_text}')\n",
        "            print('---')"
      ],
      "metadata": {
        "id": "vT6j29Xd-hyp",
        "outputId": "05c3ba13-fd2a-4890-833d-bfceae5a5cd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: False\n",
            "Correct: Sareen doolaara kuma afur nyaattee maatii harka qullaatti hambiste.\n",
            "---\n",
            "Predicted: False\n",
            "Correct: Sareen doolaara kuma afur nyaatee maatii harka qullaatti hambise.\n",
            "---\n",
            "Predicted: Otoo si maal gootu?\n",
            "Correct: Otoo isinii maal gootu?\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}