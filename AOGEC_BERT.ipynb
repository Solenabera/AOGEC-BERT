{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwAl/an0X67RodkGehScnr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Solenabera/AOGEC-BERT/blob/main/AOGEC_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ybsGCJ1Jbrtx",
        "outputId": "a3c12b58-f2f1-4cc0-e3d4-67ec34701e3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TIj4RigEJ62G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the T5 model and tokenizer\n",
        "model_name = \"t5-base\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "iWjLELnZK_yQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4986b341-4a1c-48b2-c1a4-74f303ff814f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define your custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.data.iloc[index]\n",
        "        input_text = example['input']\n",
        "        target_text = example['target']\n",
        "\n",
        "        input_ids = self.tokenizer.encode(input_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        target_ids = self.tokenizer.encode(target_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids.squeeze(),\n",
        "            'attention_mask': input_ids.squeeze().gt(0),\n",
        "            'labels': target_ids.squeeze()\n",
        "        }"
      ],
      "metadata": {
        "id": "uSFnMn-NZ8vx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 3: Split your dataset into training and validation sets\n",
        "csv_file_path = 'datasets.csv'\n",
        "dataset = pd.read_csv(csv_file_path, encoding='latin1')\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.05, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.05, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", len(train_data))\n",
        "print(\"Validation set size:\", len(val_data))\n",
        "print(\"Testing set size:\", len(test_data))"
      ],
      "metadata": {
        "id": "vGg_Mveog71x",
        "outputId": "55f518b7-0c55-42b9-f729-38ea8520806f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 2790\n",
            "Validation set size: 147\n",
            "Testing set size: 155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create custom datasets for training and validation\n",
        "train_dataset = CustomDataset(train_data, tokenizer)\n",
        "val_dataset = CustomDataset(val_data, tokenizer)\n",
        "test_dataset = CustomDataset(test_data, tokenizer)\n",
        "\n",
        "print(\"Training set size:\", len(train_dataset))\n",
        "print(\"Validation set size:\", len(val_dataset))\n",
        "print(\"Testing set size:\", len(test_dataset))\n",
        "\n",
        "print(val_dataset)"
      ],
      "metadata": {
        "id": "I_82PwlLhKxA",
        "outputId": "fe51d4ea-aada-4c76-eda0-e37e4cbb8d12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 2790\n",
            "Validation set size: 147\n",
            "Testing set size: 155\n",
            "<__main__.CustomDataset object at 0x79bfb12ee260>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Create data loaders for training and validation\n",
        "batch_size = 4  # Increase the batch size\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "tjGrRBpvwdjL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Measure the loss function before training\n",
        "model.eval()\n",
        "\n",
        "total_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "average_loss = total_loss / len(val_dataloader)\n",
        "print(f'Loss before training: {average_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANfsIlHUwjKH",
        "outputId": "4693f8a7-01e8-45bc-b2af-80eae5296bbd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss before training: 17.9575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Fine-tune the model\n",
        "model.train()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "epochs = 2 # Default 10\n",
        "print(\"Training Started\")\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch: {epoch}')\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print(total_loss)\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch+1} - Average Loss: {average_loss:.4f}')\n",
        "\n",
        "print(\"Training Ended\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tI7BEtvzbOj",
        "outputId": "004cb128-627e-4b85-a08c-d2ea45922526"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Started\n",
            "Epoch: 0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15.47948169708252\n",
            "29.773025512695312\n",
            "43.663923263549805\n",
            "57.9139928817749\n",
            "72.38865852355957\n",
            "85.60444259643555\n",
            "98.66634845733643\n",
            "110.78164100646973\n",
            "122.39564990997314\n",
            "133.96028995513916\n",
            "146.2484951019287\n",
            "156.76288890838623\n",
            "166.50126552581787\n",
            "176.4260959625244\n",
            "186.47482681274414\n",
            "196.6559076309204\n",
            "205.7172975540161\n",
            "215.16497898101807\n",
            "224.0402135848999\n",
            "233.1852512359619\n",
            "242.31921768188477\n",
            "250.15030145645142\n",
            "257.94907569885254\n",
            "265.96845626831055\n",
            "274.47233295440674\n",
            "281.6438479423523\n",
            "288.948118686676\n",
            "297.51538133621216\n",
            "305.2451319694519\n",
            "311.8370966911316\n",
            "320.0324749946594\n",
            "327.40478324890137\n",
            "333.38630867004395\n",
            "339.78901386260986\n",
            "345.86026525497437\n",
            "351.19479990005493\n",
            "358.13169288635254\n",
            "363.85344982147217\n",
            "369.0247507095337\n",
            "374.421413898468\n",
            "380.04563188552856\n",
            "384.5905661582947\n",
            "389.53480863571167\n",
            "394.01132917404175\n",
            "399.2731103897095\n",
            "403.28811264038086\n",
            "407.2824983596802\n",
            "411.70711040496826\n",
            "415.79375171661377\n",
            "419.37093114852905\n",
            "423.1544508934021\n",
            "426.6487236022949\n",
            "429.0176384449005\n",
            "432.17870712280273\n",
            "434.4214463233948\n",
            "437.01265954971313\n",
            "439.7335321903229\n",
            "442.41147017478943\n",
            "444.8661208152771\n",
            "447.5296697616577\n",
            "450.19025349617004\n",
            "452.09425032138824\n",
            "454.2605298757553\n",
            "455.9560134410858\n",
            "457.65274155139923\n",
            "459.1685199737549\n",
            "461.3037745952606\n",
            "462.54212963581085\n",
            "464.42248928546906\n",
            "466.27911698818207\n",
            "467.8215857744217\n",
            "468.9001512527466\n",
            "469.95858931541443\n",
            "470.93053621053696\n",
            "472.28983372449875\n",
            "473.4914924502373\n",
            "474.812697827816\n",
            "475.9551917910576\n",
            "476.8713355064392\n",
            "477.9539989233017\n",
            "478.7687287926674\n",
            "481.5020278096199\n",
            "482.57931131124496\n",
            "483.59317249059677\n",
            "484.4754446744919\n",
            "485.5270611047745\n",
            "486.1728057861328\n",
            "487.1272313594818\n",
            "488.3637236356735\n",
            "489.35766357183456\n",
            "490.1740223169327\n",
            "491.17864990234375\n",
            "492.167465031147\n",
            "492.9990617632866\n",
            "494.10738784074783\n",
            "494.93182051181793\n",
            "496.1102851629257\n",
            "497.18922233581543\n",
            "498.2415351867676\n",
            "499.1089932322502\n",
            "499.9191806316376\n",
            "500.54514449834824\n",
            "501.7306099534035\n",
            "502.6840969324112\n",
            "503.41493248939514\n",
            "504.3951072692871\n",
            "505.12292271852493\n",
            "505.8474676012993\n",
            "506.80466800928116\n",
            "507.7233023047447\n",
            "508.67987221479416\n",
            "510.1783817410469\n",
            "511.48923379182816\n",
            "512.2434389591217\n",
            "512.900945007801\n",
            "514.0212354063988\n",
            "514.7873222231865\n",
            "515.5376765727997\n",
            "516.2654279470444\n",
            "516.9053092598915\n",
            "517.5946065783501\n",
            "518.5624732375145\n",
            "519.2655599713326\n",
            "520.4101583361626\n",
            "521.0544542074203\n",
            "521.8274139165878\n",
            "522.6233338713646\n",
            "523.2897467017174\n",
            "523.9882467985153\n",
            "524.6162381768227\n",
            "525.2965712547302\n",
            "525.9011169075966\n",
            "526.5740996599197\n",
            "527.3623173236847\n",
            "528.143203496933\n",
            "528.740221619606\n",
            "529.3557552099228\n",
            "529.9889092445374\n",
            "530.732368350029\n",
            "531.4053859114647\n",
            "531.9473957419395\n",
            "532.9632888436317\n",
            "533.7071377038956\n",
            "534.3092630505562\n",
            "534.828818500042\n",
            "535.2993938922882\n",
            "535.8459774851799\n",
            "536.4446058273315\n",
            "537.202230155468\n",
            "537.7025018930435\n",
            "538.2194339036942\n",
            "538.8966335654259\n",
            "539.4724473953247\n",
            "540.1962045431137\n",
            "540.5859077274799\n",
            "541.3079996407032\n",
            "541.9609334766865\n",
            "542.4410852193832\n",
            "542.9731608629227\n",
            "543.5992588400841\n",
            "544.3350830078125\n",
            "545.1148263812065\n",
            "545.7369744181633\n",
            "546.3461940288544\n",
            "546.9617652893066\n",
            "547.6575731635094\n",
            "548.225527703762\n",
            "548.8817363381386\n",
            "549.5707823634148\n",
            "549.9701877236366\n",
            "550.6183249354362\n",
            "551.2143449783325\n",
            "551.673319965601\n",
            "552.1755713522434\n",
            "552.7391355931759\n",
            "553.2257898747921\n",
            "553.6960054934025\n",
            "554.3625477254391\n",
            "554.9105775654316\n",
            "555.4743199050426\n",
            "555.938862323761\n",
            "556.5437525510788\n",
            "556.9912834763527\n",
            "557.5874603390694\n",
            "557.9938535094261\n",
            "558.3887100219727\n",
            "558.9298761487007\n",
            "559.4323840141296\n",
            "559.9076128005981\n",
            "560.3384576141834\n",
            "560.9064698517323\n",
            "561.2676659524441\n",
            "561.6347550153732\n",
            "562.1881135106087\n",
            "562.7031497359276\n",
            "563.1816713809967\n",
            "563.6587899029255\n",
            "564.0813649892807\n",
            "564.6238944530487\n",
            "565.2246387600899\n",
            "565.6710216999054\n",
            "566.171759724617\n",
            "566.7125815153122\n",
            "567.1342335939407\n",
            "567.7083582282066\n",
            "568.121343255043\n",
            "568.5512796938419\n",
            "569.0243088304996\n",
            "569.5053186118603\n",
            "569.9126622080803\n",
            "570.3896899819374\n",
            "570.8747157454491\n",
            "571.3020508885384\n",
            "571.638158172369\n",
            "572.1266325414181\n",
            "572.5686754882336\n",
            "573.0510698854923\n",
            "573.454587072134\n",
            "573.8221905529499\n",
            "574.2073665261269\n",
            "574.641443669796\n",
            "575.0179240703583\n",
            "575.4092773497105\n",
            "575.8925577700138\n",
            "576.3142519295216\n",
            "576.6722269356251\n",
            "577.0955226421356\n",
            "577.4562779664993\n",
            "577.8909204900265\n",
            "578.442693978548\n",
            "578.814589202404\n",
            "579.1709267497063\n",
            "579.5242097377777\n",
            "579.9248959422112\n",
            "580.3563954532146\n",
            "580.7036362290382\n",
            "581.1144164204597\n",
            "581.5368303060532\n",
            "582.0246274471283\n",
            "582.4146881699562\n",
            "582.8312791883945\n",
            "583.171954780817\n",
            "583.5368211865425\n",
            "584.0261291861534\n",
            "584.4428741633892\n",
            "584.9363987147808\n",
            "585.3542186021805\n",
            "585.7355059683323\n",
            "586.3146027028561\n",
            "586.680502384901\n",
            "586.9677919447422\n",
            "587.3090803921223\n",
            "587.7792128622532\n",
            "588.0956391096115\n",
            "588.456958681345\n",
            "588.8907426893711\n",
            "589.2068087160587\n",
            "589.6297563016415\n",
            "590.0257898569107\n",
            "590.5465897917747\n",
            "590.8575283288956\n",
            "591.2124832570553\n",
            "591.5505917966366\n",
            "591.8693383932114\n",
            "592.2167282104492\n",
            "592.5855184793472\n",
            "592.9284380674362\n",
            "593.364286839962\n",
            "593.7958472073078\n",
            "594.1641620099545\n",
            "594.6279751062393\n",
            "594.9712798893452\n",
            "595.2874925136566\n",
            "595.6414480507374\n",
            "595.9573867619038\n",
            "596.2518802285194\n",
            "596.6244873404503\n",
            "596.9808900654316\n",
            "597.2767345607281\n",
            "597.6167079508305\n",
            "597.986572176218\n",
            "598.4983448088169\n",
            "598.8475212752819\n",
            "599.2140158414841\n",
            "599.6580802500248\n",
            "600.0493239462376\n",
            "600.3897579014301\n",
            "600.7405967116356\n",
            "601.1560156345367\n",
            "601.5300803780556\n",
            "601.9555461704731\n",
            "602.3718752264977\n",
            "602.7383428514004\n",
            "603.0863294303417\n",
            "603.5668368637562\n",
            "603.9842674732208\n",
            "604.3544058203697\n",
            "604.711987555027\n",
            "605.0368613004684\n",
            "605.41373026371\n",
            "605.7161444723606\n",
            "606.0777629315853\n",
            "606.4878918528557\n",
            "606.9063818156719\n",
            "607.2550781965256\n",
            "607.6078799068928\n",
            "607.9829568564892\n",
            "608.4231516420841\n",
            "608.7654131948948\n",
            "609.1253181099892\n",
            "609.4908449351788\n",
            "609.7916021943092\n",
            "610.034201413393\n",
            "610.4561659395695\n",
            "610.7690235674381\n",
            "611.1223531663418\n",
            "611.4382258653641\n",
            "611.6696549355984\n",
            "611.9373648762703\n",
            "612.2248204946518\n",
            "612.6421614587307\n",
            "612.9828155636787\n",
            "613.2624763250351\n",
            "613.6545306146145\n",
            "614.017695069313\n",
            "614.3454575240612\n",
            "614.7282589375973\n",
            "615.0812518000603\n",
            "615.3921248316765\n",
            "615.7724879682064\n",
            "616.0745947957039\n",
            "616.3177905380726\n",
            "616.65090700984\n",
            "616.8918318450451\n",
            "617.1846477687359\n",
            "617.6337628364563\n",
            "617.9115053117275\n",
            "618.3234170973301\n",
            "618.8762633502483\n",
            "619.1431131660938\n",
            "619.4281024038792\n",
            "619.7450523078442\n",
            "620.0585697591305\n",
            "620.4445975422859\n",
            "620.8923383653164\n",
            "621.2972484230995\n",
            "621.6549848914146\n",
            "621.9899265468121\n",
            "622.3296138942242\n",
            "622.6399734914303\n",
            "622.9702621102333\n",
            "623.2657122612\n",
            "623.6035290062428\n",
            "623.980291903019\n",
            "624.2682111859322\n",
            "624.54382199049\n",
            "624.9008655250072\n",
            "625.2883513271809\n",
            "625.6243130862713\n",
            "625.9043700695038\n",
            "626.1668783128262\n",
            "626.4467134773731\n",
            "626.7924180030823\n",
            "627.0411386936903\n",
            "627.3398277908564\n",
            "627.6806150227785\n",
            "628.0305657535791\n",
            "628.3236846476793\n",
            "628.7238435596228\n",
            "629.0492135435343\n",
            "629.4355803281069\n",
            "629.7904484421015\n",
            "630.1111883074045\n",
            "630.4017930775881\n",
            "630.8016146868467\n",
            "631.1142845898867\n",
            "631.3500853478909\n",
            "631.6955388188362\n",
            "632.023632735014\n",
            "632.2695329785347\n",
            "632.5942076146603\n",
            "632.9058057665825\n",
            "633.1515627354383\n",
            "633.4817143529654\n",
            "633.7490949779749\n",
            "634.1101162284613\n",
            "634.3692566901445\n",
            "634.6462853699923\n",
            "634.8323940187693\n",
            "635.1864995509386\n",
            "635.4723269790411\n",
            "635.8774634450674\n",
            "636.1768513768911\n",
            "636.429809525609\n",
            "636.7919999212027\n",
            "637.1620303541422\n",
            "637.447292253375\n",
            "637.7655363231897\n",
            "638.1993293613195\n",
            "638.5497547239065\n",
            "638.8458607047796\n",
            "639.1498951762915\n",
            "639.4916773587465\n",
            "639.7283092737198\n",
            "639.9776808619499\n",
            "640.3322400152683\n",
            "640.6067898273468\n",
            "640.8353588879108\n",
            "641.0889762938023\n",
            "641.4122206568718\n",
            "641.6991572082043\n",
            "641.9596315026283\n",
            "642.2602444887161\n",
            "642.4852992594242\n",
            "642.8699821233749\n",
            "643.230720937252\n",
            "643.5794796645641\n",
            "643.9523870646954\n",
            "644.2585760354996\n",
            "644.5946388542652\n",
            "644.8776963353157\n",
            "645.1351349055767\n",
            "645.3779577761889\n",
            "645.6842126101255\n",
            "645.9524952918291\n",
            "646.1962161660194\n",
            "646.469790071249\n",
            "646.6995757222176\n",
            "647.0171620547771\n",
            "647.2287284582853\n",
            "647.5382207483053\n",
            "647.7942255288363\n",
            "648.115415379405\n",
            "648.3853118270636\n",
            "648.7946775704622\n",
            "649.0907969325781\n",
            "649.3519818931818\n",
            "649.6471349745989\n",
            "649.9540328532457\n",
            "650.2252182811499\n",
            "650.4331518560648\n",
            "650.7176800519228\n",
            "651.0421988815069\n",
            "651.3059329539537\n",
            "651.5921097844839\n",
            "651.9214510768652\n",
            "652.2254842370749\n",
            "652.4938063770533\n",
            "652.7183634638786\n",
            "652.9518092423677\n",
            "653.2301390916109\n",
            "653.504571184516\n",
            "653.7062841653824\n",
            "653.9319566190243\n",
            "654.1686507463455\n",
            "654.3924444168806\n",
            "654.6136358231306\n",
            "654.8543850183487\n",
            "655.1695830225945\n",
            "655.4613087177277\n",
            "655.6535508781672\n",
            "655.8863887935877\n",
            "656.2031641453505\n",
            "656.4689126461744\n",
            "656.7259159833193\n",
            "656.9982924610376\n",
            "657.218069344759\n",
            "657.47209456563\n",
            "657.77587723732\n",
            "658.0771184265614\n",
            "658.3290562033653\n",
            "658.4970950633287\n",
            "658.791187569499\n",
            "659.1147832125425\n",
            "659.3464633375406\n",
            "659.6004160195589\n",
            "659.8478420078754\n",
            "660.1369661688805\n",
            "660.3865111321211\n",
            "660.6584870368242\n",
            "660.9201578348875\n",
            "661.1310839056969\n",
            "661.3034448027611\n",
            "661.5746076107025\n",
            "661.835706859827\n",
            "662.0811450779438\n",
            "662.4183092713356\n",
            "662.651841968298\n",
            "662.8996203243732\n",
            "663.1241599917412\n",
            "663.4410580992699\n",
            "663.6423199772835\n",
            "663.9691653847694\n",
            "664.2169543206692\n",
            "664.4295624494553\n",
            "664.7570813596249\n",
            "665.0077462792397\n",
            "665.3052394986153\n",
            "665.6041744947433\n",
            "665.7916860729456\n",
            "666.0351485013962\n",
            "666.2595405876637\n",
            "666.2595405876637\n",
            "666.4465156942606\n",
            "666.4465156942606\n",
            "666.7454160302877\n",
            "666.7454160302877\n",
            "666.9723110944033\n",
            "666.9723110944033\n",
            "667.1976042985916\n",
            "667.1976042985916\n",
            "667.4228656589985\n",
            "667.4228656589985\n",
            "667.7062283456326\n",
            "667.7062283456326\n",
            "667.9989074468613\n",
            "667.9989074468613\n",
            "668.3092873394489\n",
            "668.3092873394489\n",
            "668.5469847768545\n",
            "668.5469847768545\n",
            "668.7922346144915\n",
            "668.7922346144915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Measure the loss function after training\n",
        "model.eval()\n",
        "\n",
        "total_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "average_loss = total_loss / len(val_dataloader)\n",
        "print(f'Loss after training: {average_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87mYKAie_o4V",
        "outputId": "9850ae99-b9b4-4ccc-82d5-fbcd475f1ab7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after training: 8.2627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Save the fine-tuned model\n",
        "model.save_pretrained('OutputModel')"
      ],
      "metadata": {
        "id": "dfI2T_wH_1EM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Generate grammar error corrections\n",
        "model = T5ForConditionalGeneration.from_pretrained('OutputModel')\n",
        "model.eval()\n",
        "\n",
        "predicted_texts = []\n",
        "correct_texts = []\n",
        "test_inputs = []\n",
        "\n",
        "batch_size = 1\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "output_file = 'Testing_Data_Output.txt'\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Generate corrections\n",
        "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128, num_beams=4)\n",
        "        predicted_ids = outputs\n",
        "\n",
        "        # Decode predictions into texts\n",
        "        predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
        "        correct_text = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
        "\n",
        "        # Store predictions, correct texts, and test inputs\n",
        "        predicted_texts.append(predicted_text)\n",
        "        correct_texts.append(correct_text)\n",
        "        test_inputs.extend(input_texts)\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for test_input, predicted_text, correct_text in zip(test_inputs, predicted_texts, correct_texts):\n",
        "        print(f'Test Input: {test_input}')\n",
        "        print(f'Predicted correction: {predicted_text}')\n",
        "        print(f'Correct One: {correct_text}')\n",
        "\n",
        "        f.write(f'Test Input: {test_input}\\n')\n",
        "        f.write(f'Predicted correction: {predicted_text}\\n')\n",
        "        f.write(f'Correct One: {correct_text}\\n')\n",
        "\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "TWLGjT3nBF8O",
        "outputId": "cee3f32c-b54c-4086-9a62-71d87a5e4812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Input: grammar: Firoomsaan Leenca ajjeste.\n",
            "Predicted correction: grammar: Firoomsaan Leenca ajjeste.\n",
            "Correct One: Firoomsaan Leenca ajjeese.\n",
            "Test Input: grammar: Isaan Leenca ajjeeste.\n",
            "Predicted correction: grammar: Isaan Leenca ajjeeste.\n",
            "Correct One: Isaan Leenca ajjeesan.\n",
            "Test Input: grammar: Inni kaleessa dhufu.\n",
            "Predicted correction: grammar: Inni kaleessa dhufu.\n",
            "Correct One: Inni kaleessa dhufe.\n",
            "Test Input: grammar: Isaan ulee cabsite.\n",
            "Predicted correction: grammar: Isaan ulee cabsite.\n",
            "Correct One: Isaan ulee cabsan.\n",
            "Test Input: grammar: Guutaan kaleessa deema.\n",
            "Predicted correction: grammar: Guutaan kaleessa deema.\n",
            "Correct One: Guutaan kaleessa deeme.\n",
            "Test Input: grammar: Isheen muka muran.\n",
            "Predicted correction: grammar: Isheen muka muran.\n",
            "Correct One: Isheen muka murte.\n",
            "Test Input: grammar: Isaan kaleessa dhufte.\n",
            "Predicted correction: grammar: Isaan kaleessa dhufte.\n",
            "Correct One: Isaan kaleessa dhufan.\n",
            "Test Input: grammar: Caaltuun muka mure.\n",
            "Predicted correction: grammar: Caaltuun muka mure.\n",
            "Correct One: Caaltuun muka murte.\n",
            "Test Input: grammar: Leensaan nu barsiisa.\n",
            "Predicted correction: grammar: Leensaan nu barsiisa.\n",
            "Correct One: Leensaan nu barsiisti.\n",
            "Test Input: grammar: Isaan muka mure.\n",
            "Predicted correction: grammar: Isaan muka mure.\n",
            "Correct One: Isaan muka muran.\n",
            "Test Input: grammar: Isaan ulee cabse.\n",
            "Predicted correction: grammar: Isaan ulee cabse.\n",
            "Correct One: Isaan ulee cabsan.\n",
            "Test Input: grammar: Ani ulee cabsan.\n",
            "Predicted correction: grammar: Ani ulee cabsan.\n",
            "Correct One: Ani ulee cabse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_precision_recall_f1(reference_sentences, hypothesis_sentences):\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    for reference, hypothesis in zip(reference_sentences, hypothesis_sentences):\n",
        "        # Split the sentences into words for comparison\n",
        "        reference_words = reference.split()\n",
        "        hypothesis_words = hypothesis.split()\n",
        "\n",
        "        # Calculate true positives, false positives, and false negatives\n",
        "        for word in hypothesis_words:\n",
        "            if word in reference_words:\n",
        "                true_positives += 1\n",
        "                reference_words.remove(word)\n",
        "            else:\n",
        "                false_positives += 1\n",
        "\n",
        "        false_negatives += len(reference_words)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "    f1_score = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "# Remove the 'grammar: ' prefix from predicted sentences\n",
        "predicted_sentences = [sentence.split(': ')[1] for sentence in predicted_texts]\n",
        "\n",
        "# Remove the '.' at the end of predicted sentences\n",
        "predicted_sentences = [sentence[:-1] for sentence in predicted_sentences]\n",
        "\n",
        "precision, recall, f1_score = calculate_precision_recall_f1(correct_texts, predicted_sentences)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1_score)"
      ],
      "metadata": {
        "id": "0NM9OqpLD3nG",
        "outputId": "7050f93f-1f97-4cfe-f07c-4dde15f6a402",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6666666666666666\n",
            "F1-score: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qKAPQT-4IPsZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}